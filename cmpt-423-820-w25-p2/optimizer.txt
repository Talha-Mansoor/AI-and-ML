
Training Baseline Model (Logistic Regression)...

[Parameters] Epochs: 50  Learning Rate: 0.001  Batch Size: 64  Optimizer: SGD
[Epoch 1] Loss: 0.7672182321548462
[Epoch 2] Loss: 0.6190050840377808
[Epoch 3] Loss: 0.5551682114601135
[Epoch 4] Loss: 0.5180156230926514
[Epoch 5] Loss: 0.49331891536712646
[Epoch 6] Loss: 0.47558051347732544
[Epoch 7] Loss: 0.4621526598930359
[Epoch 8] Loss: 0.4515833258628845
[Epoch 9] Loss: 0.443004846572876
[Epoch 10] Loss: 0.4358660578727722
[Epoch 11] Loss: 0.42980051040649414
[Epoch 12] Loss: 0.42455554008483887
[Epoch 13] Loss: 0.419951468706131
[Epoch 14] Loss: 0.4158570468425751
[Epoch 15] Loss: 0.41217535734176636
[Epoch 16] Loss: 0.4088325798511505
[Epoch 17] Loss: 0.4057721495628357
[Epoch 18] Loss: 0.4029492735862732
[Epoch 19] Loss: 0.4003291130065918
[Epoch 20] Loss: 0.39788326621055603
[Epoch 21] Loss: 0.3955893814563751
[Epoch 22] Loss: 0.3934282064437866
[Epoch 23] Loss: 0.3913849890232086
[Epoch 24] Loss: 0.3894463777542114
[Epoch 25] Loss: 0.38760197162628174
[Epoch 26] Loss: 0.38584256172180176
[Epoch 27] Loss: 0.38415998220443726
[Epoch 28] Loss: 0.38254767656326294
[Epoch 29] Loss: 0.38099998235702515
[Epoch 30] Loss: 0.3795117139816284
[Epoch 31] Loss: 0.3780785799026489
[Epoch 32] Loss: 0.3766964077949524
[Epoch 33] Loss: 0.37536200881004333
[Epoch 34] Loss: 0.3740721344947815
[Epoch 35] Loss: 0.37282419204711914
[Epoch 36] Loss: 0.3716156482696533
[Epoch 37] Loss: 0.37044399976730347
[Epoch 38] Loss: 0.36930757761001587
[Epoch 39] Loss: 0.36820441484451294
[Epoch 40] Loss: 0.3671327829360962
[Epoch 41] Loss: 0.3660909831523895
[Epoch 42] Loss: 0.3650778830051422
[Epoch 43] Loss: 0.3640918433666229
[Epoch 44] Loss: 0.3631317615509033
[Epoch 45] Loss: 0.36219677329063416
[Epoch 46] Loss: 0.36128538846969604
[Epoch 47] Loss: 0.3603966236114502
[Epoch 48] Loss: 0.359529584646225
[Epoch 49] Loss: 0.3586835265159607
[Epoch 50] Loss: 0.3578575849533081
Test Accuracy: 83.99%

Training Baseline Model (Logistic Regression)...

[Parameters] Epochs: 50  Learning Rate: 0.001  Batch Size: 64  Optimizer: Adam
[Epoch 1] Loss: 0.4355319142341614
[Epoch 2] Loss: 0.3897586464881897
[Epoch 3] Loss: 0.3665849566459656
[Epoch 4] Loss: 0.3508259057998657
[Epoch 5] Loss: 0.3386479616165161
[Epoch 6] Loss: 0.3288140296936035
[Epoch 7] Loss: 0.3206545114517212
[Epoch 8] Loss: 0.3137452006340027
[Epoch 9] Loss: 0.30779939889907837
[Epoch 10] Loss: 0.3026120662689209
[Epoch 11] Loss: 0.29803428053855896
[Epoch 12] Loss: 0.2939564883708954
[Epoch 13] Loss: 0.2902960777282715
[Epoch 14] Loss: 0.2869868278503418
[Epoch 15] Loss: 0.28397640585899353
[Epoch 16] Loss: 0.2812219262123108
[Epoch 17] Loss: 0.2786889672279358
[Epoch 18] Loss: 0.2763502299785614
[Epoch 19] Loss: 0.27418258786201477
[Epoch 20] Loss: 0.27216804027557373
[Epoch 21] Loss: 0.270290732383728
[Epoch 22] Loss: 0.2685372829437256
[Epoch 23] Loss: 0.26689743995666504
[Epoch 24] Loss: 0.265360951423645
[Epoch 25] Loss: 0.2639196813106537
[Epoch 26] Loss: 0.26256614923477173
[Epoch 27] Loss: 0.261293888092041
[Epoch 28] Loss: 0.26009657979011536
[Epoch 29] Loss: 0.2589690685272217
[Epoch 30] Loss: 0.25790655612945557
[Epoch 31] Loss: 0.2569043040275574
[Epoch 32] Loss: 0.2559584975242615
[Epoch 33] Loss: 0.25506484508514404
[Epoch 34] Loss: 0.25421977043151855
[Epoch 35] Loss: 0.25342023372650146
[Epoch 36] Loss: 0.2526629865169525
[Epoch 37] Loss: 0.25194504857063293
[Epoch 38] Loss: 0.2512642443180084
[Epoch 39] Loss: 0.25061744451522827
[Epoch 40] Loss: 0.25000235438346863
[Epoch 41] Loss: 0.24941706657409668
[Epoch 42] Loss: 0.24885961413383484
[Epoch 43] Loss: 0.24832814931869507
[Epoch 44] Loss: 0.24782079458236694
[Epoch 45] Loss: 0.2473360002040863
[Epoch 46] Loss: 0.24687254428863525
[Epoch 47] Loss: 0.2464287281036377
[Epoch 48] Loss: 0.24600371718406677
[Epoch 49] Loss: 0.24559606611728668
[Epoch 50] Loss: 0.2452048361301422
Test Accuracy: 82.07%
